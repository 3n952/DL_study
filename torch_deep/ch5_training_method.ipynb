{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0,6.0, 13.0,21.0]\n",
    "y = [35.7, 55.9,58.2,81.9,56.3,48.9,33.9,21.8,48.4,60.4,68.4]\n",
    "\n",
    "print(len(x) == len(y))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형관계를 가정\n",
    "def linear_model(x,w,b):\n",
    "    return w * x + b\n",
    "\n",
    "def loss_fn(p, x):\n",
    "    squared_diff = (p-x)**2\n",
    "    return squared_diff.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w,b 초기화\n",
    "\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = linear_model(x,w,b)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(p,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "# w에 대한 손실함수의 기울기\n",
    "w_change_rate = (loss_fn(linear_model(x, w+ delta, b), y) / 2.0 * delta) \n",
    "b_change_rate = (loss_fn(linear_model(x, w, b+delta), y) / 2.0 * delta) \n",
    "\n",
    "lr = 1e-2\n",
    "w = w - lr * w_change_rate\n",
    "b = b - lr * b_change_rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-989.5273,  -82.6000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미분 계산\n",
    "# 각 식의 도함수로 표현\n",
    "\n",
    "def dloss_fn(p, y):\n",
    "    dsq_diffs = 2 * (p-y) / p.size(0)\n",
    "    return dsq_diffs\n",
    "\n",
    "def dw(x,w,b):\n",
    "    return x\n",
    "\n",
    "def db(x,w,b):\n",
    "    return 1.0\n",
    "\n",
    "#미분 계산한 함수\n",
    "def grad_fn(x, y, p, w, b):\n",
    "    dloss_ftp = dloss_fn(p, y)\n",
    "    dloss_dw = dloss_ftp * dw(x,w,b)\n",
    "    dloss_db = dloss_ftp * db(x,w,b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "\n",
    "result = grad_fn(x,y,p,w,b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "params: tensor([1.0990, 0.0083])\n",
      "grad: tensor([-989.5273,  -82.6000])\n",
      "Epoch 2, Loss 1667.137939\n",
      "params: tensor([1.1942, 0.0163])\n",
      "grad: tensor([-952.2690,  -80.5055])\n",
      "Epoch 3, Loss 1577.523804\n",
      "params: tensor([1.2858, 0.0242])\n",
      "grad: tensor([-916.4115,  -78.4896])\n",
      "Epoch 4, Loss 1494.515503\n",
      "params: tensor([1.3740, 0.0318])\n",
      "grad: tensor([-881.9021,  -76.5494])\n",
      "Epoch 5, Loss 1417.625977\n",
      "params: tensor([1.4589, 0.0393])\n",
      "grad: tensor([-848.6901,  -74.6821])\n",
      "Epoch 6, Loss 1346.403809\n",
      "params: tensor([1.5406, 0.0466])\n",
      "grad: tensor([-816.7266,  -72.8850])\n",
      "Epoch 7, Loss 1280.430786\n",
      "params: tensor([1.6191, 0.0537])\n",
      "grad: tensor([-785.9648,  -71.1552])\n",
      "Epoch 8, Loss 1219.319824\n",
      "params: tensor([1.6948, 0.0606])\n",
      "grad: tensor([-756.3597,  -69.4905])\n",
      "Epoch 9, Loss 1162.712036\n",
      "params: tensor([1.7676, 0.0674])\n",
      "grad: tensor([-727.8675,  -67.8882])\n",
      "Epoch 10, Loss 1110.275146\n",
      "params: tensor([1.8376, 0.0741])\n",
      "grad: tensor([-700.4465,  -66.3461])\n",
      "Epoch 11, Loss 1061.701660\n",
      "params: tensor([1.9050, 0.0805])\n",
      "grad: tensor([-674.0562,  -64.8619])\n",
      "Epoch 12, Loss 1016.706421\n",
      "params: tensor([1.9699, 0.0869])\n",
      "grad: tensor([-648.6583,  -63.4334])\n",
      "Epoch 13, Loss 975.025330\n",
      "params: tensor([2.0323, 0.0931])\n",
      "grad: tensor([-624.2151,  -62.0586])\n",
      "Epoch 14, Loss 936.414062\n",
      "params: tensor([2.0924, 0.0992])\n",
      "grad: tensor([-600.6909,  -60.7353])\n",
      "Epoch 15, Loss 900.646118\n",
      "params: tensor([2.1502, 0.1051])\n",
      "grad: tensor([-578.0511,  -59.4617])\n",
      "Epoch 16, Loss 867.511658\n",
      "params: tensor([2.2058, 0.1109])\n",
      "grad: tensor([-556.2624,  -58.2359])\n",
      "Epoch 17, Loss 836.816406\n",
      "params: tensor([2.2593, 0.1166])\n",
      "grad: tensor([-535.2930,  -57.0561])\n",
      "Epoch 18, Loss 808.380432\n",
      "params: tensor([2.3109, 0.1222])\n",
      "grad: tensor([-515.1118,  -55.9206])\n",
      "Epoch 19, Loss 782.036926\n",
      "params: tensor([2.3604, 0.1277])\n",
      "grad: tensor([-495.6894,  -54.8277])\n",
      "Epoch 20, Loss 757.631775\n",
      "params: tensor([2.4081, 0.1331])\n",
      "grad: tensor([-476.9972,  -53.7758])\n",
      "Epoch 21, Loss 735.021667\n",
      "params: tensor([2.4540, 0.1384])\n",
      "grad: tensor([-459.0078,  -52.7633])\n",
      "Epoch 22, Loss 714.074402\n",
      "params: tensor([2.4982, 0.1436])\n",
      "grad: tensor([-441.6945,  -51.7888])\n",
      "Epoch 23, Loss 694.667175\n",
      "params: tensor([2.5407, 0.1486])\n",
      "grad: tensor([-425.0323,  -50.8509])\n",
      "Epoch 24, Loss 676.686462\n",
      "params: tensor([2.5816, 0.1536])\n",
      "grad: tensor([-408.9965,  -49.9482])\n",
      "Epoch 25, Loss 660.026917\n",
      "params: tensor([2.6209, 0.1585])\n",
      "grad: tensor([-393.5635,  -49.0793])\n",
      "Epoch 26, Loss 644.591187\n",
      "params: tensor([2.6588, 0.1634])\n",
      "grad: tensor([-378.7108,  -48.2430])\n",
      "Epoch 27, Loss 630.288879\n",
      "params: tensor([2.6953, 0.1681])\n",
      "grad: tensor([-364.4164,  -47.4381])\n",
      "Epoch 28, Loss 617.036438\n",
      "params: tensor([2.7303, 0.1728])\n",
      "grad: tensor([-350.6595,  -46.6633])\n",
      "Epoch 29, Loss 604.756348\n",
      "params: tensor([2.7641, 0.1774])\n",
      "grad: tensor([-337.4197,  -45.9176])\n",
      "Epoch 30, Loss 593.376892\n",
      "params: tensor([2.7965, 0.1819])\n",
      "grad: tensor([-324.6777,  -45.1998])\n",
      "Epoch 31, Loss 582.831726\n",
      "params: tensor([2.8278, 0.1863])\n",
      "grad: tensor([-312.4148,  -44.5090])\n",
      "Epoch 32, Loss 573.059143\n",
      "params: tensor([2.8578, 0.1907])\n",
      "grad: tensor([-300.6129,  -43.8440])\n",
      "Epoch 33, Loss 564.002258\n",
      "params: tensor([2.8868, 0.1950])\n",
      "grad: tensor([-289.2547,  -43.2039])\n",
      "Epoch 34, Loss 555.608154\n",
      "params: tensor([2.9146, 0.1993])\n",
      "grad: tensor([-278.3235,  -42.5879])\n",
      "Epoch 35, Loss 547.828064\n",
      "params: tensor([2.9414, 0.2035])\n",
      "grad: tensor([-267.8032,  -41.9949])\n",
      "Epoch 36, Loss 540.616638\n",
      "params: tensor([2.9671, 0.2076])\n",
      "grad: tensor([-257.6785,  -41.4241])\n",
      "Epoch 37, Loss 533.931885\n",
      "params: tensor([2.9919, 0.2117])\n",
      "grad: tensor([-247.9344,  -40.8747])\n",
      "Epoch 38, Loss 527.735107\n",
      "params: tensor([3.0158, 0.2158])\n",
      "grad: tensor([-238.5568,  -40.3458])\n",
      "Epoch 39, Loss 521.990051\n",
      "params: tensor([3.0387, 0.2197])\n",
      "grad: tensor([-229.5316,  -39.8368])\n",
      "Epoch 40, Loss 516.663513\n",
      "params: tensor([3.0608, 0.2237])\n",
      "grad: tensor([-220.8456,  -39.3468])\n",
      "Epoch 41, Loss 511.724609\n",
      "params: tensor([3.0821, 0.2276])\n",
      "grad: tensor([-212.4864,  -38.8752])\n",
      "Epoch 42, Loss 507.144836\n",
      "params: tensor([3.1025, 0.2314])\n",
      "grad: tensor([-204.4413,  -38.4212])\n",
      "Epoch 43, Loss 502.897583\n",
      "params: tensor([3.1222, 0.2352])\n",
      "grad: tensor([-196.6986,  -37.9842])\n",
      "Epoch 44, Loss 498.958374\n",
      "params: tensor([3.1411, 0.2390])\n",
      "grad: tensor([-189.2473,  -37.5635])\n",
      "Epoch 45, Loss 495.304413\n",
      "params: tensor([3.1593, 0.2427])\n",
      "grad: tensor([-182.0759,  -37.1586])\n",
      "Epoch 46, Loss 491.914764\n",
      "params: tensor([3.1768, 0.2464])\n",
      "grad: tensor([-175.1742,  -36.7688])\n",
      "Epoch 47, Loss 488.769806\n",
      "params: tensor([3.1937, 0.2500])\n",
      "grad: tensor([-168.5319,  -36.3936])\n",
      "Epoch 48, Loss 485.851654\n",
      "params: tensor([3.2099, 0.2536])\n",
      "grad: tensor([-162.1394,  -36.0324])\n",
      "Epoch 49, Loss 483.143433\n",
      "params: tensor([3.2255, 0.2572])\n",
      "grad: tensor([-155.9873,  -35.6847])\n",
      "Epoch 50, Loss 480.629578\n",
      "params: tensor([3.2405, 0.2607])\n",
      "grad: tensor([-150.0663,  -35.3499])\n",
      "Epoch 51, Loss 478.295990\n",
      "params: tensor([3.2550, 0.2642])\n",
      "grad: tensor([-144.3681,  -35.0277])\n",
      "Epoch 52, Loss 476.129211\n",
      "params: tensor([3.2688, 0.2677])\n",
      "grad: tensor([-138.8840,  -34.7176])\n",
      "Epoch 53, Loss 474.116974\n",
      "params: tensor([3.2822, 0.2711])\n",
      "grad: tensor([-133.6061,  -34.4190])\n",
      "Epoch 54, Loss 472.247864\n",
      "params: tensor([3.2951, 0.2745])\n",
      "grad: tensor([-128.5267,  -34.1315])\n",
      "Epoch 55, Loss 470.511353\n",
      "params: tensor([3.3074, 0.2779])\n",
      "grad: tensor([-123.6381,  -33.8548])\n",
      "Epoch 56, Loss 468.897675\n",
      "params: tensor([3.3193, 0.2813])\n",
      "grad: tensor([-118.9334,  -33.5884])\n",
      "Epoch 57, Loss 467.397675\n",
      "params: tensor([3.3308, 0.2846])\n",
      "grad: tensor([-114.4056,  -33.3319])\n",
      "Epoch 58, Loss 466.003052\n",
      "params: tensor([3.3418, 0.2879])\n",
      "grad: tensor([-110.0480,  -33.0850])\n",
      "Epoch 59, Loss 464.706024\n",
      "params: tensor([3.3523, 0.2912])\n",
      "grad: tensor([-105.8543,  -32.8473])\n",
      "Epoch 60, Loss 463.499298\n",
      "params: tensor([3.3625, 0.2945])\n",
      "grad: tensor([-101.8181,  -32.6184])\n",
      "Epoch 61, Loss 462.376373\n",
      "params: tensor([3.3723, 0.2977])\n",
      "grad: tensor([-97.9337, -32.3981])\n",
      "Epoch 62, Loss 461.330963\n",
      "params: tensor([3.3817, 0.3009])\n",
      "grad: tensor([-94.1955, -32.1859])\n",
      "Epoch 63, Loss 460.357391\n",
      "params: tensor([3.3908, 0.3041])\n",
      "grad: tensor([-90.5977, -31.9817])\n",
      "Epoch 64, Loss 459.450287\n",
      "params: tensor([3.3995, 0.3073])\n",
      "grad: tensor([-87.1353, -31.7850])\n",
      "Epoch 65, Loss 458.604889\n",
      "params: tensor([3.4079, 0.3105])\n",
      "grad: tensor([-83.8029, -31.5957])\n",
      "Epoch 66, Loss 457.816437\n",
      "params: tensor([3.4160, 0.3136])\n",
      "grad: tensor([-80.5958, -31.4134])\n",
      "Epoch 67, Loss 457.080933\n",
      "params: tensor([3.4237, 0.3167])\n",
      "grad: tensor([-77.5094, -31.2378])\n",
      "Epoch 68, Loss 456.394409\n",
      "params: tensor([3.4312, 0.3198])\n",
      "grad: tensor([-74.5390, -31.0688])\n",
      "Epoch 69, Loss 455.753113\n",
      "params: tensor([3.4383, 0.3229])\n",
      "grad: tensor([-71.6802, -30.9061])\n",
      "Epoch 70, Loss 455.153839\n",
      "params: tensor([3.4452, 0.3260])\n",
      "grad: tensor([-68.9290, -30.7494])\n",
      "Epoch 71, Loss 454.593536\n",
      "params: tensor([3.4518, 0.3291])\n",
      "grad: tensor([-66.2811, -30.5985])\n",
      "Epoch 72, Loss 454.069336\n",
      "params: tensor([3.4582, 0.3321])\n",
      "grad: tensor([-63.7328, -30.4532])\n",
      "Epoch 73, Loss 453.578491\n",
      "params: tensor([3.4643, 0.3351])\n",
      "grad: tensor([-61.2804, -30.3132])\n",
      "Epoch 74, Loss 453.118439\n",
      "params: tensor([3.4702, 0.3382])\n",
      "grad: tensor([-58.9201, -30.1785])\n",
      "Epoch 75, Loss 452.687134\n",
      "params: tensor([3.4759, 0.3412])\n",
      "grad: tensor([-56.6486, -30.0487])\n",
      "Epoch 76, Loss 452.282318\n",
      "params: tensor([3.4813, 0.3442])\n",
      "grad: tensor([-54.4624, -29.9237])\n",
      "Epoch 77, Loss 451.902069\n",
      "params: tensor([3.4866, 0.3471])\n",
      "grad: tensor([-52.3584, -29.8034])\n",
      "Epoch 78, Loss 451.544525\n",
      "params: tensor([3.4916, 0.3501])\n",
      "grad: tensor([-50.3336, -29.6875])\n",
      "Epoch 79, Loss 451.208099\n",
      "params: tensor([3.4965, 0.3531])\n",
      "grad: tensor([-48.3849, -29.5758])\n",
      "Epoch 80, Loss 450.891235\n",
      "params: tensor([3.5011, 0.3560])\n",
      "grad: tensor([-46.5095, -29.4683])\n",
      "Epoch 81, Loss 450.592468\n",
      "params: tensor([3.5056, 0.3589])\n",
      "grad: tensor([-44.7046, -29.3648])\n",
      "Epoch 82, Loss 450.310455\n",
      "params: tensor([3.5099, 0.3619])\n",
      "grad: tensor([-42.9676, -29.2650])\n",
      "Epoch 83, Loss 450.043854\n",
      "params: tensor([3.5140, 0.3648])\n",
      "grad: tensor([-41.2958, -29.1689])\n",
      "Epoch 84, Loss 449.791718\n",
      "params: tensor([3.5180, 0.3677])\n",
      "grad: tensor([-39.6869, -29.0764])\n",
      "Epoch 85, Loss 449.552856\n",
      "params: tensor([3.5218, 0.3706])\n",
      "grad: tensor([-38.1385, -28.9872])\n",
      "Epoch 86, Loss 449.326294\n",
      "params: tensor([3.5255, 0.3735])\n",
      "grad: tensor([-36.6483, -28.9013])\n",
      "Epoch 87, Loss 449.111237\n",
      "params: tensor([3.5290, 0.3764])\n",
      "grad: tensor([-35.2142, -28.8186])\n",
      "Epoch 88, Loss 448.906769\n",
      "params: tensor([3.5324, 0.3792])\n",
      "grad: tensor([-33.8339, -28.7389])\n",
      "Epoch 89, Loss 448.712036\n",
      "params: tensor([3.5356, 0.3821])\n",
      "grad: tensor([-32.5056, -28.6621])\n",
      "Epoch 90, Loss 448.526398\n",
      "params: tensor([3.5387, 0.3850])\n",
      "grad: tensor([-31.2271, -28.5881])\n",
      "Epoch 91, Loss 448.349243\n",
      "params: tensor([3.5417, 0.3878])\n",
      "grad: tensor([-29.9967, -28.5168])\n",
      "Epoch 92, Loss 448.179779\n",
      "params: tensor([3.5446, 0.3907])\n",
      "grad: tensor([-28.8126, -28.4481])\n",
      "Epoch 93, Loss 448.017548\n",
      "params: tensor([3.5474, 0.3935])\n",
      "grad: tensor([-27.6730, -28.3819])\n",
      "Epoch 94, Loss 447.862030\n",
      "params: tensor([3.5500, 0.3963])\n",
      "grad: tensor([-26.5763, -28.3181])\n",
      "Epoch 95, Loss 447.712769\n",
      "params: tensor([3.5526, 0.3992])\n",
      "grad: tensor([-25.5208, -28.2566])\n",
      "Epoch 96, Loss 447.569061\n",
      "params: tensor([3.5550, 0.4020])\n",
      "grad: tensor([-24.5051, -28.1974])\n",
      "Epoch 97, Loss 447.430756\n",
      "params: tensor([3.5574, 0.4048])\n",
      "grad: tensor([-23.5276, -28.1403])\n",
      "Epoch 98, Loss 447.297394\n",
      "params: tensor([3.5596, 0.4076])\n",
      "grad: tensor([-22.5867, -28.0852])\n",
      "Epoch 99, Loss 447.168579\n",
      "params: tensor([3.5618, 0.4104])\n",
      "grad: tensor([-21.6811, -28.0322])\n",
      "Epoch 100, Loss 447.044067\n",
      "params: tensor([3.5639, 0.4132])\n",
      "grad: tensor([-20.8097, -27.9810])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.5639, 0.4132])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train loop\n",
    "\n",
    "def training_loop(epochs, lr,params, x, y):\n",
    "    for epoch in range(1, epochs +1):\n",
    "        w, b = params\n",
    "        p = linear_model(x,w,b)\n",
    "        loss = loss_fn(p, y)\n",
    "        grad = grad_fn(x, y, p, w, b)\n",
    "        params = params - lr * grad\n",
    "\n",
    "        print('Epoch %d, Loss %f'%(epoch, float(loss)))\n",
    "        print('params:', params)\n",
    "        print('grad:', grad)\n",
    "    \n",
    "    return params\n",
    "\n",
    "training_loop(100, 1e-4, torch.tensor([1.0, 0.0]), x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 2806.242188\n",
      "params: tensor([1.0133, 0.0101])\n",
      "grad: tensor([-132.6823, -101.5000])\n",
      "Epoch 2, Loss 2803.452148\n",
      "params: tensor([1.0265, 0.0203])\n",
      "grad: tensor([-132.6112, -101.4518])\n",
      "Epoch 3, Loss 2800.665039\n",
      "params: tensor([1.0398, 0.0304])\n",
      "grad: tensor([-132.5402, -101.4037])\n",
      "Epoch 4, Loss 2797.881104\n",
      "params: tensor([1.0530, 0.0406])\n",
      "grad: tensor([-132.4693, -101.3556])\n",
      "Epoch 5, Loss 2795.099365\n",
      "params: tensor([1.0663, 0.0507])\n",
      "grad: tensor([-132.3983, -101.3075])\n",
      "Epoch 6, Loss 2792.320801\n",
      "params: tensor([1.0795, 0.0608])\n",
      "grad: tensor([-132.3274, -101.2594])\n",
      "Epoch 7, Loss 2789.545166\n",
      "params: tensor([1.0927, 0.0709])\n",
      "grad: tensor([-132.2566, -101.2114])\n",
      "Epoch 8, Loss 2786.772461\n",
      "params: tensor([1.1059, 0.0811])\n",
      "grad: tensor([-132.1858, -101.1634])\n",
      "Epoch 9, Loss 2784.002197\n",
      "params: tensor([1.1192, 0.0912])\n",
      "grad: tensor([-132.1150, -101.1154])\n",
      "Epoch 10, Loss 2781.235352\n",
      "params: tensor([1.1324, 0.1013])\n",
      "grad: tensor([-132.0443, -101.0674])\n",
      "Epoch 11, Loss 2778.470947\n",
      "params: tensor([1.1456, 0.1114])\n",
      "grad: tensor([-131.9735, -101.0195])\n",
      "Epoch 12, Loss 2775.709473\n",
      "params: tensor([1.1588, 0.1215])\n",
      "grad: tensor([-131.9029, -100.9716])\n",
      "Epoch 13, Loss 2772.950928\n",
      "params: tensor([1.1719, 0.1316])\n",
      "grad: tensor([-131.8322, -100.9237])\n",
      "Epoch 14, Loss 2770.194824\n",
      "params: tensor([1.1851, 0.1417])\n",
      "grad: tensor([-131.7616, -100.8758])\n",
      "Epoch 15, Loss 2767.442139\n",
      "params: tensor([1.1983, 0.1517])\n",
      "grad: tensor([-131.6911, -100.8279])\n",
      "Epoch 16, Loss 2764.692139\n",
      "params: tensor([1.2114, 0.1618])\n",
      "grad: tensor([-131.6205, -100.7801])\n",
      "Epoch 17, Loss 2761.944336\n",
      "params: tensor([1.2246, 0.1719])\n",
      "grad: tensor([-131.5500, -100.7323])\n",
      "Epoch 18, Loss 2759.199707\n",
      "params: tensor([1.2377, 0.1820])\n",
      "grad: tensor([-131.4796, -100.6846])\n",
      "Epoch 19, Loss 2756.458008\n",
      "params: tensor([1.2509, 0.1920])\n",
      "grad: tensor([-131.4091, -100.6368])\n",
      "Epoch 20, Loss 2753.718994\n",
      "params: tensor([1.2640, 0.2021])\n",
      "grad: tensor([-131.3388, -100.5891])\n",
      "Epoch 21, Loss 2750.983398\n",
      "params: tensor([1.2771, 0.2121])\n",
      "grad: tensor([-131.2684, -100.5414])\n",
      "Epoch 22, Loss 2748.249756\n",
      "params: tensor([1.2903, 0.2222])\n",
      "grad: tensor([-131.1981, -100.4937])\n",
      "Epoch 23, Loss 2745.519287\n",
      "params: tensor([1.3034, 0.2322])\n",
      "grad: tensor([-131.1278, -100.4461])\n",
      "Epoch 24, Loss 2742.791748\n",
      "params: tensor([1.3165, 0.2423])\n",
      "grad: tensor([-131.0576, -100.3984])\n",
      "Epoch 25, Loss 2740.066650\n",
      "params: tensor([1.3296, 0.2523])\n",
      "grad: tensor([-130.9874, -100.3508])\n",
      "Epoch 26, Loss 2737.344727\n",
      "params: tensor([1.3427, 0.2623])\n",
      "grad: tensor([-130.9172, -100.3033])\n",
      "Epoch 27, Loss 2734.625488\n",
      "params: tensor([1.3558, 0.2724])\n",
      "grad: tensor([-130.8471, -100.2557])\n",
      "Epoch 28, Loss 2731.908936\n",
      "params: tensor([1.3688, 0.2824])\n",
      "grad: tensor([-130.7770, -100.2082])\n",
      "Epoch 29, Loss 2729.195312\n",
      "params: tensor([1.3819, 0.2924])\n",
      "grad: tensor([-130.7070, -100.1607])\n",
      "Epoch 30, Loss 2726.484131\n",
      "params: tensor([1.3950, 0.3024])\n",
      "grad: tensor([-130.6369, -100.1132])\n",
      "Epoch 31, Loss 2723.775879\n",
      "params: tensor([1.4080, 0.3124])\n",
      "grad: tensor([-130.5670, -100.0657])\n",
      "Epoch 32, Loss 2721.070557\n",
      "params: tensor([1.4211, 0.3224])\n",
      "grad: tensor([-130.4970, -100.0183])\n",
      "Epoch 33, Loss 2718.368164\n",
      "params: tensor([1.4341, 0.3324])\n",
      "grad: tensor([-130.4271,  -99.9709])\n",
      "Epoch 34, Loss 2715.668213\n",
      "params: tensor([1.4472, 0.3424])\n",
      "grad: tensor([-130.3572,  -99.9235])\n",
      "Epoch 35, Loss 2712.970947\n",
      "params: tensor([1.4602, 0.3524])\n",
      "grad: tensor([-130.2874,  -99.8762])\n",
      "Epoch 36, Loss 2710.277100\n",
      "params: tensor([1.4732, 0.3624])\n",
      "grad: tensor([-130.2176,  -99.8288])\n",
      "Epoch 37, Loss 2707.585205\n",
      "params: tensor([1.4862, 0.3724])\n",
      "grad: tensor([-130.1478,  -99.7815])\n",
      "Epoch 38, Loss 2704.896240\n",
      "params: tensor([1.4992, 0.3823])\n",
      "grad: tensor([-130.0781,  -99.7342])\n",
      "Epoch 39, Loss 2702.210693\n",
      "params: tensor([1.5122, 0.3923])\n",
      "grad: tensor([-130.0084,  -99.6870])\n",
      "Epoch 40, Loss 2699.527100\n",
      "params: tensor([1.5252, 0.4023])\n",
      "grad: tensor([-129.9387,  -99.6397])\n",
      "Epoch 41, Loss 2696.846191\n",
      "params: tensor([1.5382, 0.4122])\n",
      "grad: tensor([-129.8691,  -99.5925])\n",
      "Epoch 42, Loss 2694.168457\n",
      "params: tensor([1.5512, 0.4222])\n",
      "grad: tensor([-129.7995,  -99.5453])\n",
      "Epoch 43, Loss 2691.493896\n",
      "params: tensor([1.5642, 0.4321])\n",
      "grad: tensor([-129.7300,  -99.4981])\n",
      "Epoch 44, Loss 2688.821289\n",
      "params: tensor([1.5771, 0.4421])\n",
      "grad: tensor([-129.6605,  -99.4510])\n",
      "Epoch 45, Loss 2686.151611\n",
      "params: tensor([1.5901, 0.4520])\n",
      "grad: tensor([-129.5910,  -99.4039])\n",
      "Epoch 46, Loss 2683.485107\n",
      "params: tensor([1.6030, 0.4620])\n",
      "grad: tensor([-129.5216,  -99.3568])\n",
      "Epoch 47, Loss 2680.821045\n",
      "params: tensor([1.6160, 0.4719])\n",
      "grad: tensor([-129.4521,  -99.3097])\n",
      "Epoch 48, Loss 2678.159424\n",
      "params: tensor([1.6289, 0.4818])\n",
      "grad: tensor([-129.3828,  -99.2627])\n",
      "Epoch 49, Loss 2675.500977\n",
      "params: tensor([1.6419, 0.4917])\n",
      "grad: tensor([-129.3134,  -99.2156])\n",
      "Epoch 50, Loss 2672.844971\n",
      "params: tensor([1.6548, 0.5016])\n",
      "grad: tensor([-129.2442,  -99.1686])\n",
      "Epoch 51, Loss 2670.192139\n",
      "params: tensor([1.6677, 0.5116])\n",
      "grad: tensor([-129.1749,  -99.1217])\n",
      "Epoch 52, Loss 2667.541504\n",
      "params: tensor([1.6806, 0.5215])\n",
      "grad: tensor([-129.1057,  -99.0747])\n",
      "Epoch 53, Loss 2664.893555\n",
      "params: tensor([1.6935, 0.5314])\n",
      "grad: tensor([-129.0365,  -99.0278])\n",
      "Epoch 54, Loss 2662.249023\n",
      "params: tensor([1.7064, 0.5413])\n",
      "grad: tensor([-128.9673,  -98.9809])\n",
      "Epoch 55, Loss 2659.606445\n",
      "params: tensor([1.7193, 0.5512])\n",
      "grad: tensor([-128.8982,  -98.9340])\n",
      "Epoch 56, Loss 2656.967041\n",
      "params: tensor([1.7322, 0.5611])\n",
      "grad: tensor([-128.8291,  -98.8871])\n",
      "Epoch 57, Loss 2654.329834\n",
      "params: tensor([1.7451, 0.5709])\n",
      "grad: tensor([-128.7600,  -98.8403])\n",
      "Epoch 58, Loss 2651.696045\n",
      "params: tensor([1.7579, 0.5808])\n",
      "grad: tensor([-128.6910,  -98.7935])\n",
      "Epoch 59, Loss 2649.064453\n",
      "params: tensor([1.7708, 0.5907])\n",
      "grad: tensor([-128.6221,  -98.7467])\n",
      "Epoch 60, Loss 2646.435547\n",
      "params: tensor([1.7836, 0.6006])\n",
      "grad: tensor([-128.5531,  -98.7000])\n",
      "Epoch 61, Loss 2643.809326\n",
      "params: tensor([1.7965, 0.6104])\n",
      "grad: tensor([-128.4842,  -98.6532])\n",
      "Epoch 62, Loss 2641.186279\n",
      "params: tensor([1.8093, 0.6203])\n",
      "grad: tensor([-128.4153,  -98.6065])\n",
      "Epoch 63, Loss 2638.565430\n",
      "params: tensor([1.8222, 0.6301])\n",
      "grad: tensor([-128.3465,  -98.5598])\n",
      "Epoch 64, Loss 2635.947266\n",
      "params: tensor([1.8350, 0.6400])\n",
      "grad: tensor([-128.2777,  -98.5132])\n",
      "Epoch 65, Loss 2633.332031\n",
      "params: tensor([1.8478, 0.6498])\n",
      "grad: tensor([-128.2090,  -98.4665])\n",
      "Epoch 66, Loss 2630.719238\n",
      "params: tensor([1.8606, 0.6597])\n",
      "grad: tensor([-128.1402,  -98.4199])\n",
      "Epoch 67, Loss 2628.109131\n",
      "params: tensor([1.8734, 0.6695])\n",
      "grad: tensor([-128.0715,  -98.3733])\n",
      "Epoch 68, Loss 2625.502197\n",
      "params: tensor([1.8862, 0.6794])\n",
      "grad: tensor([-128.0029,  -98.3268])\n",
      "Epoch 69, Loss 2622.897461\n",
      "params: tensor([1.8990, 0.6892])\n",
      "grad: tensor([-127.9343,  -98.2802])\n",
      "Epoch 70, Loss 2620.295410\n",
      "params: tensor([1.9118, 0.6990])\n",
      "grad: tensor([-127.8657,  -98.2337])\n",
      "Epoch 71, Loss 2617.696289\n",
      "params: tensor([1.9246, 0.7088])\n",
      "grad: tensor([-127.7971,  -98.1872])\n",
      "Epoch 72, Loss 2615.099854\n",
      "params: tensor([1.9374, 0.7186])\n",
      "grad: tensor([-127.7286,  -98.1407])\n",
      "Epoch 73, Loss 2612.505615\n",
      "params: tensor([1.9501, 0.7284])\n",
      "grad: tensor([-127.6601,  -98.0943])\n",
      "Epoch 74, Loss 2609.914307\n",
      "params: tensor([1.9629, 0.7382])\n",
      "grad: tensor([-127.5917,  -98.0478])\n",
      "Epoch 75, Loss 2607.325684\n",
      "params: tensor([1.9756, 0.7480])\n",
      "grad: tensor([-127.5233,  -98.0014])\n",
      "Epoch 76, Loss 2604.739746\n",
      "params: tensor([1.9884, 0.7578])\n",
      "grad: tensor([-127.4549,  -97.9551])\n",
      "Epoch 77, Loss 2602.156494\n",
      "params: tensor([2.0011, 0.7676])\n",
      "grad: tensor([-127.3866,  -97.9087])\n",
      "Epoch 78, Loss 2599.575684\n",
      "params: tensor([2.0139, 0.7774])\n",
      "grad: tensor([-127.3183,  -97.8624])\n",
      "Epoch 79, Loss 2596.997803\n",
      "params: tensor([2.0266, 0.7872])\n",
      "grad: tensor([-127.2500,  -97.8160])\n",
      "Epoch 80, Loss 2594.422119\n",
      "params: tensor([2.0393, 0.7970])\n",
      "grad: tensor([-127.1818,  -97.7698])\n",
      "Epoch 81, Loss 2591.849365\n",
      "params: tensor([2.0520, 0.8068])\n",
      "grad: tensor([-127.1136,  -97.7235])\n",
      "Epoch 82, Loss 2589.279297\n",
      "params: tensor([2.0647, 0.8165])\n",
      "grad: tensor([-127.0454,  -97.6773])\n",
      "Epoch 83, Loss 2586.711914\n",
      "params: tensor([2.0774, 0.8263])\n",
      "grad: tensor([-126.9773,  -97.6310])\n",
      "Epoch 84, Loss 2584.147217\n",
      "params: tensor([2.0901, 0.8360])\n",
      "grad: tensor([-126.9092,  -97.5848])\n",
      "Epoch 85, Loss 2581.584961\n",
      "params: tensor([2.1028, 0.8458])\n",
      "grad: tensor([-126.8411,  -97.5387])\n",
      "Epoch 86, Loss 2579.025146\n",
      "params: tensor([2.1155, 0.8555])\n",
      "grad: tensor([-126.7731,  -97.4925])\n",
      "Epoch 87, Loss 2576.468262\n",
      "params: tensor([2.1281, 0.8653])\n",
      "grad: tensor([-126.7051,  -97.4464])\n",
      "Epoch 88, Loss 2573.913818\n",
      "params: tensor([2.1408, 0.8750])\n",
      "grad: tensor([-126.6372,  -97.4003])\n",
      "Epoch 89, Loss 2571.362305\n",
      "params: tensor([2.1535, 0.8848])\n",
      "grad: tensor([-126.5693,  -97.3542])\n",
      "Epoch 90, Loss 2568.813477\n",
      "params: tensor([2.1661, 0.8945])\n",
      "grad: tensor([-126.5014,  -97.3082])\n",
      "Epoch 91, Loss 2566.266602\n",
      "params: tensor([2.1788, 0.9042])\n",
      "grad: tensor([-126.4336,  -97.2622])\n",
      "Epoch 92, Loss 2563.722656\n",
      "params: tensor([2.1914, 0.9139])\n",
      "grad: tensor([-126.3657,  -97.2162])\n",
      "Epoch 93, Loss 2561.181396\n",
      "params: tensor([2.2040, 0.9237])\n",
      "grad: tensor([-126.2980,  -97.1702])\n",
      "Epoch 94, Loss 2558.642822\n",
      "params: tensor([2.2166, 0.9334])\n",
      "grad: tensor([-126.2302,  -97.1242])\n",
      "Epoch 95, Loss 2556.106445\n",
      "params: tensor([2.2293, 0.9431])\n",
      "grad: tensor([-126.1625,  -97.0783])\n",
      "Epoch 96, Loss 2553.573242\n",
      "params: tensor([2.2419, 0.9528])\n",
      "grad: tensor([-126.0949,  -97.0324])\n",
      "Epoch 97, Loss 2551.042725\n",
      "params: tensor([2.2545, 0.9625])\n",
      "grad: tensor([-126.0272,  -96.9865])\n",
      "Epoch 98, Loss 2548.513916\n",
      "params: tensor([2.2671, 0.9722])\n",
      "grad: tensor([-125.9596,  -96.9406])\n",
      "Epoch 99, Loss 2545.988037\n",
      "params: tensor([2.2797, 0.9819])\n",
      "grad: tensor([-125.8921,  -96.8948])\n",
      "Epoch 100, Loss 2543.465332\n",
      "params: tensor([2.2922, 0.9916])\n",
      "grad: tensor([-125.8245,  -96.8490])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2922, 0.9916])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = 0.1 * x\n",
    "training_loop(100, 1e-4, torch.tensor([1.0, 0.0]), x2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "params: tensor([1.0990, 0.0083], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-989.5273,  -82.6000], grad_fn=<StackBackward0>)\n",
      "Epoch 2, Loss 1667.137939\n",
      "params: tensor([1.1942, 0.0163], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-952.2690,  -80.5055], grad_fn=<StackBackward0>)\n",
      "Epoch 3, Loss 1577.523804\n",
      "params: tensor([1.2858, 0.0242], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-916.4115,  -78.4896], grad_fn=<StackBackward0>)\n",
      "Epoch 4, Loss 1494.515503\n",
      "params: tensor([1.3740, 0.0318], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-881.9021,  -76.5494], grad_fn=<StackBackward0>)\n",
      "Epoch 5, Loss 1417.625977\n",
      "params: tensor([1.4589, 0.0393], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-848.6901,  -74.6821], grad_fn=<StackBackward0>)\n",
      "Epoch 6, Loss 1346.403809\n",
      "params: tensor([1.5406, 0.0466], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-816.7266,  -72.8850], grad_fn=<StackBackward0>)\n",
      "Epoch 7, Loss 1280.430786\n",
      "params: tensor([1.6191, 0.0537], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-785.9648,  -71.1552], grad_fn=<StackBackward0>)\n",
      "Epoch 8, Loss 1219.319824\n",
      "params: tensor([1.6948, 0.0606], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-756.3597,  -69.4905], grad_fn=<StackBackward0>)\n",
      "Epoch 9, Loss 1162.712036\n",
      "params: tensor([1.7676, 0.0674], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-727.8675,  -67.8882], grad_fn=<StackBackward0>)\n",
      "Epoch 10, Loss 1110.275146\n",
      "params: tensor([1.8376, 0.0741], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-700.4465,  -66.3461], grad_fn=<StackBackward0>)\n",
      "Epoch 11, Loss 1061.701660\n",
      "params: tensor([1.9050, 0.0805], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-674.0562,  -64.8619], grad_fn=<StackBackward0>)\n",
      "Epoch 12, Loss 1016.706421\n",
      "params: tensor([1.9699, 0.0869], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-648.6583,  -63.4334], grad_fn=<StackBackward0>)\n",
      "Epoch 13, Loss 975.025330\n",
      "params: tensor([2.0323, 0.0931], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-624.2151,  -62.0586], grad_fn=<StackBackward0>)\n",
      "Epoch 14, Loss 936.414062\n",
      "params: tensor([2.0924, 0.0992], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-600.6909,  -60.7353], grad_fn=<StackBackward0>)\n",
      "Epoch 15, Loss 900.646118\n",
      "params: tensor([2.1502, 0.1051], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-578.0511,  -59.4617], grad_fn=<StackBackward0>)\n",
      "Epoch 16, Loss 867.511658\n",
      "params: tensor([2.2058, 0.1109], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-556.2624,  -58.2359], grad_fn=<StackBackward0>)\n",
      "Epoch 17, Loss 836.816406\n",
      "params: tensor([2.2593, 0.1166], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-535.2930,  -57.0561], grad_fn=<StackBackward0>)\n",
      "Epoch 18, Loss 808.380432\n",
      "params: tensor([2.3109, 0.1222], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-515.1118,  -55.9206], grad_fn=<StackBackward0>)\n",
      "Epoch 19, Loss 782.036926\n",
      "params: tensor([2.3604, 0.1277], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-495.6894,  -54.8277], grad_fn=<StackBackward0>)\n",
      "Epoch 20, Loss 757.631775\n",
      "params: tensor([2.4081, 0.1331], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-476.9972,  -53.7758], grad_fn=<StackBackward0>)\n",
      "Epoch 21, Loss 735.021667\n",
      "params: tensor([2.4540, 0.1384], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-459.0078,  -52.7633], grad_fn=<StackBackward0>)\n",
      "Epoch 22, Loss 714.074402\n",
      "params: tensor([2.4982, 0.1436], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-441.6945,  -51.7888], grad_fn=<StackBackward0>)\n",
      "Epoch 23, Loss 694.667175\n",
      "params: tensor([2.5407, 0.1486], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-425.0323,  -50.8509], grad_fn=<StackBackward0>)\n",
      "Epoch 24, Loss 676.686462\n",
      "params: tensor([2.5816, 0.1536], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-408.9965,  -49.9482], grad_fn=<StackBackward0>)\n",
      "Epoch 25, Loss 660.026917\n",
      "params: tensor([2.6209, 0.1585], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-393.5635,  -49.0793], grad_fn=<StackBackward0>)\n",
      "Epoch 26, Loss 644.591187\n",
      "params: tensor([2.6588, 0.1634], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-378.7108,  -48.2430], grad_fn=<StackBackward0>)\n",
      "Epoch 27, Loss 630.288879\n",
      "params: tensor([2.6953, 0.1681], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-364.4164,  -47.4381], grad_fn=<StackBackward0>)\n",
      "Epoch 28, Loss 617.036438\n",
      "params: tensor([2.7303, 0.1728], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-350.6595,  -46.6633], grad_fn=<StackBackward0>)\n",
      "Epoch 29, Loss 604.756348\n",
      "params: tensor([2.7641, 0.1774], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-337.4197,  -45.9176], grad_fn=<StackBackward0>)\n",
      "Epoch 30, Loss 593.376892\n",
      "params: tensor([2.7965, 0.1819], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-324.6777,  -45.1998], grad_fn=<StackBackward0>)\n",
      "Epoch 31, Loss 582.831726\n",
      "params: tensor([2.8278, 0.1863], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-312.4148,  -44.5090], grad_fn=<StackBackward0>)\n",
      "Epoch 32, Loss 573.059143\n",
      "params: tensor([2.8578, 0.1907], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-300.6129,  -43.8440], grad_fn=<StackBackward0>)\n",
      "Epoch 33, Loss 564.002258\n",
      "params: tensor([2.8868, 0.1950], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-289.2547,  -43.2039], grad_fn=<StackBackward0>)\n",
      "Epoch 34, Loss 555.608154\n",
      "params: tensor([2.9146, 0.1993], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-278.3235,  -42.5879], grad_fn=<StackBackward0>)\n",
      "Epoch 35, Loss 547.828064\n",
      "params: tensor([2.9414, 0.2035], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-267.8032,  -41.9949], grad_fn=<StackBackward0>)\n",
      "Epoch 36, Loss 540.616638\n",
      "params: tensor([2.9671, 0.2076], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-257.6785,  -41.4241], grad_fn=<StackBackward0>)\n",
      "Epoch 37, Loss 533.931885\n",
      "params: tensor([2.9919, 0.2117], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-247.9344,  -40.8747], grad_fn=<StackBackward0>)\n",
      "Epoch 38, Loss 527.735107\n",
      "params: tensor([3.0158, 0.2158], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-238.5568,  -40.3458], grad_fn=<StackBackward0>)\n",
      "Epoch 39, Loss 521.990051\n",
      "params: tensor([3.0387, 0.2197], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-229.5316,  -39.8368], grad_fn=<StackBackward0>)\n",
      "Epoch 40, Loss 516.663513\n",
      "params: tensor([3.0608, 0.2237], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-220.8456,  -39.3468], grad_fn=<StackBackward0>)\n",
      "Epoch 41, Loss 511.724609\n",
      "params: tensor([3.0821, 0.2276], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-212.4864,  -38.8752], grad_fn=<StackBackward0>)\n",
      "Epoch 42, Loss 507.144836\n",
      "params: tensor([3.1025, 0.2314], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-204.4413,  -38.4212], grad_fn=<StackBackward0>)\n",
      "Epoch 43, Loss 502.897583\n",
      "params: tensor([3.1222, 0.2352], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-196.6986,  -37.9842], grad_fn=<StackBackward0>)\n",
      "Epoch 44, Loss 498.958374\n",
      "params: tensor([3.1411, 0.2390], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-189.2473,  -37.5635], grad_fn=<StackBackward0>)\n",
      "Epoch 45, Loss 495.304413\n",
      "params: tensor([3.1593, 0.2427], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-182.0759,  -37.1586], grad_fn=<StackBackward0>)\n",
      "Epoch 46, Loss 491.914764\n",
      "params: tensor([3.1768, 0.2464], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-175.1742,  -36.7688], grad_fn=<StackBackward0>)\n",
      "Epoch 47, Loss 488.769806\n",
      "params: tensor([3.1937, 0.2500], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-168.5319,  -36.3936], grad_fn=<StackBackward0>)\n",
      "Epoch 48, Loss 485.851654\n",
      "params: tensor([3.2099, 0.2536], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-162.1394,  -36.0324], grad_fn=<StackBackward0>)\n",
      "Epoch 49, Loss 483.143433\n",
      "params: tensor([3.2255, 0.2572], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-155.9873,  -35.6847], grad_fn=<StackBackward0>)\n",
      "Epoch 50, Loss 480.629578\n",
      "params: tensor([3.2405, 0.2607], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-150.0663,  -35.3499], grad_fn=<StackBackward0>)\n",
      "Epoch 51, Loss 478.295990\n",
      "params: tensor([3.2550, 0.2642], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-144.3681,  -35.0277], grad_fn=<StackBackward0>)\n",
      "Epoch 52, Loss 476.129211\n",
      "params: tensor([3.2688, 0.2677], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-138.8840,  -34.7176], grad_fn=<StackBackward0>)\n",
      "Epoch 53, Loss 474.116974\n",
      "params: tensor([3.2822, 0.2711], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-133.6061,  -34.4190], grad_fn=<StackBackward0>)\n",
      "Epoch 54, Loss 472.247864\n",
      "params: tensor([3.2951, 0.2745], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-128.5267,  -34.1315], grad_fn=<StackBackward0>)\n",
      "Epoch 55, Loss 470.511353\n",
      "params: tensor([3.3074, 0.2779], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-123.6381,  -33.8548], grad_fn=<StackBackward0>)\n",
      "Epoch 56, Loss 468.897675\n",
      "params: tensor([3.3193, 0.2813], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-118.9334,  -33.5884], grad_fn=<StackBackward0>)\n",
      "Epoch 57, Loss 467.397675\n",
      "params: tensor([3.3308, 0.2846], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-114.4056,  -33.3319], grad_fn=<StackBackward0>)\n",
      "Epoch 58, Loss 466.003052\n",
      "params: tensor([3.3418, 0.2879], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-110.0480,  -33.0850], grad_fn=<StackBackward0>)\n",
      "Epoch 59, Loss 464.706024\n",
      "params: tensor([3.3523, 0.2912], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-105.8543,  -32.8473], grad_fn=<StackBackward0>)\n",
      "Epoch 60, Loss 463.499298\n",
      "params: tensor([3.3625, 0.2945], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-101.8181,  -32.6184], grad_fn=<StackBackward0>)\n",
      "Epoch 61, Loss 462.376373\n",
      "params: tensor([3.3723, 0.2977], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-97.9337, -32.3981], grad_fn=<StackBackward0>)\n",
      "Epoch 62, Loss 461.330963\n",
      "params: tensor([3.3817, 0.3009], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-94.1955, -32.1859], grad_fn=<StackBackward0>)\n",
      "Epoch 63, Loss 460.357391\n",
      "params: tensor([3.3908, 0.3041], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-90.5977, -31.9817], grad_fn=<StackBackward0>)\n",
      "Epoch 64, Loss 459.450287\n",
      "params: tensor([3.3995, 0.3073], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-87.1353, -31.7850], grad_fn=<StackBackward0>)\n",
      "Epoch 65, Loss 458.604889\n",
      "params: tensor([3.4079, 0.3105], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-83.8029, -31.5957], grad_fn=<StackBackward0>)\n",
      "Epoch 66, Loss 457.816437\n",
      "params: tensor([3.4160, 0.3136], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-80.5958, -31.4134], grad_fn=<StackBackward0>)\n",
      "Epoch 67, Loss 457.080933\n",
      "params: tensor([3.4237, 0.3167], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-77.5094, -31.2378], grad_fn=<StackBackward0>)\n",
      "Epoch 68, Loss 456.394409\n",
      "params: tensor([3.4312, 0.3198], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-74.5390, -31.0688], grad_fn=<StackBackward0>)\n",
      "Epoch 69, Loss 455.753113\n",
      "params: tensor([3.4383, 0.3229], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-71.6802, -30.9061], grad_fn=<StackBackward0>)\n",
      "Epoch 70, Loss 455.153839\n",
      "params: tensor([3.4452, 0.3260], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-68.9290, -30.7494], grad_fn=<StackBackward0>)\n",
      "Epoch 71, Loss 454.593536\n",
      "params: tensor([3.4518, 0.3291], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-66.2811, -30.5985], grad_fn=<StackBackward0>)\n",
      "Epoch 72, Loss 454.069336\n",
      "params: tensor([3.4582, 0.3321], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-63.7328, -30.4532], grad_fn=<StackBackward0>)\n",
      "Epoch 73, Loss 453.578491\n",
      "params: tensor([3.4643, 0.3351], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-61.2804, -30.3132], grad_fn=<StackBackward0>)\n",
      "Epoch 74, Loss 453.118439\n",
      "params: tensor([3.4702, 0.3382], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-58.9201, -30.1785], grad_fn=<StackBackward0>)\n",
      "Epoch 75, Loss 452.687134\n",
      "params: tensor([3.4759, 0.3412], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-56.6486, -30.0487], grad_fn=<StackBackward0>)\n",
      "Epoch 76, Loss 452.282318\n",
      "params: tensor([3.4813, 0.3442], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-54.4624, -29.9237], grad_fn=<StackBackward0>)\n",
      "Epoch 77, Loss 451.902069\n",
      "params: tensor([3.4866, 0.3471], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-52.3584, -29.8034], grad_fn=<StackBackward0>)\n",
      "Epoch 78, Loss 451.544525\n",
      "params: tensor([3.4916, 0.3501], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-50.3336, -29.6875], grad_fn=<StackBackward0>)\n",
      "Epoch 79, Loss 451.208099\n",
      "params: tensor([3.4965, 0.3531], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-48.3849, -29.5758], grad_fn=<StackBackward0>)\n",
      "Epoch 80, Loss 450.891235\n",
      "params: tensor([3.5011, 0.3560], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-46.5095, -29.4683], grad_fn=<StackBackward0>)\n",
      "Epoch 81, Loss 450.592468\n",
      "params: tensor([3.5056, 0.3589], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-44.7046, -29.3648], grad_fn=<StackBackward0>)\n",
      "Epoch 82, Loss 450.310455\n",
      "params: tensor([3.5099, 0.3619], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-42.9676, -29.2650], grad_fn=<StackBackward0>)\n",
      "Epoch 83, Loss 450.043854\n",
      "params: tensor([3.5140, 0.3648], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-41.2958, -29.1689], grad_fn=<StackBackward0>)\n",
      "Epoch 84, Loss 449.791718\n",
      "params: tensor([3.5180, 0.3677], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-39.6869, -29.0764], grad_fn=<StackBackward0>)\n",
      "Epoch 85, Loss 449.552856\n",
      "params: tensor([3.5218, 0.3706], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-38.1385, -28.9872], grad_fn=<StackBackward0>)\n",
      "Epoch 86, Loss 449.326294\n",
      "params: tensor([3.5255, 0.3735], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-36.6483, -28.9013], grad_fn=<StackBackward0>)\n",
      "Epoch 87, Loss 449.111237\n",
      "params: tensor([3.5290, 0.3764], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-35.2142, -28.8186], grad_fn=<StackBackward0>)\n",
      "Epoch 88, Loss 448.906769\n",
      "params: tensor([3.5324, 0.3792], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-33.8339, -28.7389], grad_fn=<StackBackward0>)\n",
      "Epoch 89, Loss 448.712036\n",
      "params: tensor([3.5356, 0.3821], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-32.5056, -28.6621], grad_fn=<StackBackward0>)\n",
      "Epoch 90, Loss 448.526398\n",
      "params: tensor([3.5387, 0.3850], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-31.2271, -28.5881], grad_fn=<StackBackward0>)\n",
      "Epoch 91, Loss 448.349243\n",
      "params: tensor([3.5417, 0.3878], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-29.9967, -28.5168], grad_fn=<StackBackward0>)\n",
      "Epoch 92, Loss 448.179779\n",
      "params: tensor([3.5446, 0.3907], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-28.8126, -28.4481], grad_fn=<StackBackward0>)\n",
      "Epoch 93, Loss 448.017548\n",
      "params: tensor([3.5474, 0.3935], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-27.6730, -28.3819], grad_fn=<StackBackward0>)\n",
      "Epoch 94, Loss 447.862030\n",
      "params: tensor([3.5500, 0.3963], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-26.5763, -28.3181], grad_fn=<StackBackward0>)\n",
      "Epoch 95, Loss 447.712769\n",
      "params: tensor([3.5526, 0.3992], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-25.5208, -28.2566], grad_fn=<StackBackward0>)\n",
      "Epoch 96, Loss 447.569061\n",
      "params: tensor([3.5550, 0.4020], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-24.5051, -28.1974], grad_fn=<StackBackward0>)\n",
      "Epoch 97, Loss 447.430756\n",
      "params: tensor([3.5574, 0.4048], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-23.5276, -28.1403], grad_fn=<StackBackward0>)\n",
      "Epoch 98, Loss 447.297394\n",
      "params: tensor([3.5596, 0.4076], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-22.5867, -28.0852], grad_fn=<StackBackward0>)\n",
      "Epoch 99, Loss 447.168579\n",
      "params: tensor([3.5618, 0.4104], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-21.6811, -28.0322], grad_fn=<StackBackward0>)\n",
      "Epoch 100, Loss 447.044067\n",
      "params: tensor([3.5639, 0.4132], grad_fn=<SubBackward0>)\n",
      "grad: tensor([-20.8097, -27.9810], grad_fn=<StackBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.5639, 0.4132], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1 = torch.tensor([1.0, 0.0], requires_grad= True)\n",
    "\n",
    "def training_loop2(epochs,params, x, y):\n",
    "    for epoch in range(1, epochs +1):\n",
    "        loss = loss_fn(linear_model(x, *params), y)\n",
    "        loss.backward()\n",
    "\n",
    "        print('Epoch %d, Loss %f'%(epoch, float(loss)))\n",
    "        \n",
    "    \n",
    "training_loop(100, 1e-4, params1, x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(params1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6048, 0.6825], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자동미분 + 옵티마이저 활용\n",
    "import torch.optim as optim\n",
    "def torch_training_loop(epochs, optimizer, params, x, y):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        p = linear_model(x, *params)\n",
    "        loss = loss_fn(p, y)\n",
    "\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad= True)\n",
    "lr = 1e-4\n",
    "optimizer = optim.SGD([params], lr = lr)\n",
    "torch_training_loop(200, optimizer, params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
