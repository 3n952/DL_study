{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c467e0",
   "metadata": {},
   "source": [
    "# 2 신경망의 수학적 구성요소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981fe42",
   "metadata": {},
   "source": [
    "## - 신경망의 데이터 표현\n",
    "\n",
    "**텐서**를 하나의 data 구성 단위로 생각한다. \n",
    "텐서는 임의의 차원을 가지는 데이터 컨테이너라고 생각하면 된다.\n",
    "여기서 임의의 차원을 랭크(rank)로 표현 할 수 있다. 가령 랭크-2 텐서는 행렬과 같다.\n",
    "\n",
    "### 각 랭크별 텐서가 어떤 값을 가지는 지 알아보자\n",
    "- 스칼라 (랭크 0)\n",
    "- 벡터 (랭크 1)\n",
    "- 행렬 (랭크 2)\n",
    "- 랭크 3 텐서\n",
    "- 랭크 4 텐서\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a9442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----sclar----\n",
      "12\n",
      "0\n",
      "----vector----\n",
      "[ 1 14 12  5]\n",
      "1\n",
      "----matrix----\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "2\n",
      "----rank-3 tensor----\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 코드로 각 텐서를 구현해보기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#scalar\n",
    "x = np.array(12)\n",
    "print('----sclar----')\n",
    "print(x)\n",
    "print(x.ndim)\n",
    "\n",
    "#vector\n",
    "x = np.array([1,14,12,5])\n",
    "print('----vector----')\n",
    "print(x)\n",
    "print(x.ndim)\n",
    "\n",
    "\n",
    "#matrix\n",
    "x = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print('----matrix----')\n",
    "print(x)\n",
    "print(x.ndim)\n",
    "\n",
    "\n",
    "#rank-3 tensor\n",
    "x = np.array([[[1,2,3,4],[5,6,7,8],[9,10,11,12]],[[1,2,3,4],[5,6,7,8],[9,10,11,12]],[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print('----rank-3 tensor----')\n",
    "print(x)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d430a4",
   "metadata": {},
   "source": [
    "## - 텐서의 핵심속성\n",
    "- 축의 개수(rank)\n",
    "- 크기(shape, 각 축의 차원의 크기)\n",
    "- 데이터 타입\n",
    "\n",
    "텐서가 몇 rank로 구성되어 있으며, 각 축마다의 크기가 어떤지 shape와 데이터가 어떤 타입으로 이뤄져 있는지 아는 것이 중요하다.\n",
    "\n",
    "넘파이 배열 구조의 텐서는 축의 개수, 크기, 데이터 타입을 구해주는 모듈이 있어 사용하기 간편하다.  \n",
    ": ndim / shape / dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1329149",
   "metadata": {},
   "source": [
    "### 랭크별 텐서의 실졔 예시를 살펴보자\n",
    "\n",
    "#### - 벡터데이터\n",
    "\n",
    "데이터 포인트를 벡터로 표현할 수 있다.  \n",
    "ex) 사람의 나이 성별 소득으로 구성된 통계 데이터는 (샘플수, 3)의 벡터로 표현이 가능하다.   \n",
    "이는 행렬로 볼 수 있겠지만 **벡터의 배열**로 본다.\n",
    "\n",
    "#### - 시퀀스 데이터\n",
    "\n",
    "시계열 데이터라고도 하며, 시간 순서에 따라 특성들이 나열된 데이터이다.    \n",
    "ex) 주식 가격 데이터 => 현재 주식, 이전 최고 최저 가격(3차원 벡터)으로 나타낸 것들을 1분 단위로 나타내면 (390,3)이 되고  \n",
    "이를 하루로 보고 250일에 대한 주식 데이터는 (250, 390, 3)의 크기를 갖는다.\n",
    "\n",
    "#### - 이미지 데이터\n",
    "\n",
    "높이 너비 컬러채널을 갖는 3차원 데이터이다. 흑백의 경우 관례적으로 채널을 1로 보고 컬러라면 rgb값인 3을 갖는다.  \n",
    "ex) 높이 너비 128 x 128을 갖는 컬러 이미지는 (128,128,3)의 크기를 갖는다.\n",
    "\n",
    "#### - 비디오 데이터\n",
    "\n",
    "이미지 데이터 + 시퀀스 데이터라고 생각하면 된다. 각 프레임의 연속적인 집합이다.  \n",
    "ex) 60초짜리 144 x 256 유튜브 비디오 클립을 초당 4프레임으로 샘플링하면 (240, 144, 256, 3)의 크기를 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cc3c5",
   "metadata": {},
   "source": [
    "## - 텐서의 연산\n",
    "\n",
    "딥러닝이 학습한 모든 변환을 수치 데이터 텐서에 적용하는 것을 **텐서 연산, 텐서 함수** 라고 한다.  \n",
    "딥러닝의 텐서 연산에는 일반적으로 다음과 같은 것들이 있다.  \n",
    "\n",
    "- 입력 텐서와 w 텐서의 점곱\n",
    "- 점곱을 만들어진 행렬과 벡터 b의 덧셈\n",
    "- relu와 같은 activation func 연산\n",
    "\n",
    "-> 이를 기하학적으로 해석 할 수 있다.   \n",
    "즉, 연산을 거치면서 데이터가 가지는 표현들이 달라지는 것이고 그 표현을 어떻게 달라지게 해야 모델을 표현할 수 있는지 아는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a7235",
   "metadata": {},
   "source": [
    "## - 그레이디언트 기반 최적화\n",
    "\n",
    "- 훈련 루프 동안 손실을 계속해서 낮춰야 하는데, 그것을 가능하게 하는 것이 그레이디언트 기반 최적화이다.\n",
    "\n",
    "y = f(x1, x2)일 때, x1, x2가 y에 미치는 영향을 보기 위해서는 편미분 값이 필요하다.  \n",
    "x1에 대한 y의 미분은 x1이 변하는 값에 대한 y값이 변하는 정도를 나타낸다.  \n",
    "가령 y = x + 1은 x가 1만큼 변하면 y도 1만큼 변한다는 의미와 일맥상통하다.\n",
    "  \n",
    "- w, b의 각각의 gradient를 구하면 각 w / b가 loss에 미치는 정도를 알 수 있다.  \n",
    "\n",
    "따라서 미분 가능한 loss 함수가 주어지면 각 w / b는 해당 값의 gradient 방향 반대로 움직이면 손실이 조금씩 감소할 것이라는 개념을  \n",
    "**확률적 경사 하강법**이라고 한다.\n",
    "\n",
    "- 확률적 경사 하강법에 의거하여 파라미터를 업데이트하기 위해서는 가중치에 대한 손실함수의 그레이디언트를 구할 수 있는 것은 연쇄법칙 덕분이다.\n",
    "\n",
    "연쇄법칙을 통해 역전파 알고리즘을 적용하면 어떤 복잡한 식의 그레이디언트라도 구할 수 있게 된다.\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "tensorflow와 같은 최신 딥러닝 프레임 워크는 이를 자동으로 수행해주는 자동미분 기능이 구현되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aecaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로의 자동미분 기능 활용하기: gradient tape()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#다차원 텐서 초기회\n",
    "w = tf.Variable(tf.random.uniform((2,2)))\n",
    "b = tf.Variable(tf.zeros((2,)))\n",
    "x = tf.random.uniform((2,2))\n",
    "\n",
    "#출력에 대한 모든 텐서 연산의 gradient 저장\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x,w) + b #텐서연산 dot\n",
    "\n",
    "#x에 대한 y의 gradient계산\n",
    "grad_of_yx = tape.gradient(y,[w,b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c516b747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.99673855, 0.99673855],\n",
       "        [0.8330504 , 0.8330504 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_of_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03014066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망의 학습 구조를 mnist로 이해해보기\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#data \n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "train_data = train_data.reshape((60000, -1)).astype(\"float32\") / 255\n",
    "test_data = test_data.reshape((10000, -1)).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f809c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_value = tf.random.uniform(w_shape, minval = 0, maxval = 1e-1)\n",
    "        self.w = tf.Variable(w_value)\n",
    "        \n",
    "        b_shape = (output_size,)\n",
    "        b_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_value)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        #정방향 pass\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)    \n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        #가중치 추출 \n",
    "        return [self.w, self.b]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c244ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential class\n",
    "# 층 연결/ layers를 리스트로 받는다.\n",
    "\n",
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    # 층의 weights [w,b]가 리스트에 담긴다.\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff694290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "\n",
    "import math \n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size = 128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        # image / batch_size를 반올림\n",
    "        self.num_batch = math.ceil(len(images) / batch_size)\n",
    "    \n",
    "    #하나의 전체 데이터셋에 대한 batch 순회\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc51346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step\n",
    "# 배치 데이터에서 가중치를 업데이트 하기.\n",
    "\n",
    "# 베치 데이터 별 가중치 업데이트 + 손실함수 값 계산\n",
    "def training_step(model, images_batch, labels_batch):\n",
    "    #업데이트를 위해 가중치를 저장할 tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(images_batch)\n",
    "        batch_loss = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, pred)\n",
    "        #배치 데이터별 loss 구하기 (평균)\n",
    "        avg_loss = tf.reduce_mean(batch_loss)\n",
    "    \n",
    "    # w/b에 대한 loss의 gradient 값들\n",
    "    gradients = tape.gradient(avg_loss, model.weights)\n",
    "    #추후에 정의할 함수: model.weights의 각 값에 gradient의 반대방향으로 더해주는 함수일 것으로 예상한다 \n",
    "    update_weights(gradients, model.weights)\n",
    "    return avg_loss\n",
    "\n",
    "#lr \n",
    "learning_rate = 1e-3\n",
    "\n",
    "def ex_update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        #tf.assign_sub => -=\n",
    "        w.assign_sub(g * learning_rate)\n",
    "\n",
    "#keras optimizer를 사용하자\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.legacy.Adam(learning_rate = learning_rate)\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c640e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size = 128):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch + 1}')\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batch):\n",
    "            # 0~ batch size 만큼 배치 데이터 생성\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = training_step(model, images_batch, labels_batch)\n",
    "            # 100번의 iter마다 손실 값 출력(= 배치 데이터 100번 돌면 손실 값출력)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f'{batch_counter + 1}번째 배치 손실: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c404777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "\n",
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size = 28*28, output_size = 512, activation = tf.nn.relu),\n",
    "    NaiveDense(input_size = 512, output_size = 10, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# 계속해서 쌓이지 않게 한다. 각 층에 대한 w,b가 고정되어야함 층마다 2개씩 4개\n",
    "assert len(model.weights) == 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fa4c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1번째 배치 손실: 6.24\n",
      "101번째 배치 손실: 0.50\n",
      "201번째 배치 손실: 0.31\n",
      "301번째 배치 손실: 0.28\n",
      "401번째 배치 손실: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x10xi1>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n",
      "1번째 배치 손실: 0.33\n",
      "101번째 배치 손실: 0.24\n",
      "201번째 배치 손실: 0.25\n",
      "301번째 배치 손실: 0.24\n",
      "401번째 배치 손실: 0.52\n",
      "epoch 3\n",
      "1번째 배치 손실: 0.18\n",
      "101번째 배치 손실: 0.19\n",
      "201번째 배치 손실: 0.23\n",
      "301번째 배치 손실: 0.24\n",
      "401번째 배치 손실: 0.46\n",
      "epoch 4\n",
      "1번째 배치 손실: 0.17\n",
      "101번째 배치 손실: 0.19\n",
      "201번째 배치 손실: 0.24\n",
      "301번째 배치 손실: 0.26\n",
      "401번째 배치 손실: 0.35\n",
      "epoch 5\n",
      "1번째 배치 손실: 0.17\n",
      "101번째 배치 손실: 0.19\n",
      "201번째 배치 손실: 0.25\n",
      "301번째 배치 손실: 0.24\n",
      "401번째 배치 손실: 0.30\n",
      "epoch 6\n",
      "1번째 배치 손실: 0.17\n",
      "101번째 배치 손실: 0.18\n",
      "201번째 배치 손실: 0.25\n",
      "301번째 배치 손실: 0.21\n",
      "401번째 배치 손실: 0.26\n",
      "epoch 7\n",
      "1번째 배치 손실: 0.16\n",
      "101번째 배치 손실: 0.17\n",
      "201번째 배치 손실: 0.23\n",
      "301번째 배치 손실: 0.18\n",
      "401번째 배치 손실: 0.20\n",
      "epoch 8\n",
      "1번째 배치 손실: 0.14\n",
      "101번째 배치 손실: 0.14\n",
      "201번째 배치 손실: 0.18\n",
      "301번째 배치 손실: 0.14\n",
      "401번째 배치 손실: 0.16\n",
      "epoch 9\n",
      "1번째 배치 손실: 0.12\n",
      "101번째 배치 손실: 0.11\n",
      "201번째 배치 손실: 0.13\n",
      "301번째 배치 손실: 0.11\n",
      "401번째 배치 손실: 0.14\n",
      "epoch 10\n",
      "1번째 배치 손실: 0.09\n",
      "101번째 배치 손실: 0.08\n",
      "201번째 배치 손실: 0.09\n",
      "301번째 배치 손실: 0.09\n",
      "401번째 배치 손실: 0.12\n",
      "epoch 11\n",
      "1번째 배치 손실: 0.06\n",
      "101번째 배치 손실: 0.06\n",
      "201번째 배치 손실: 0.07\n",
      "301번째 배치 손실: 0.07\n",
      "401번째 배치 손실: 0.11\n",
      "epoch 12\n",
      "1번째 배치 손실: 0.05\n",
      "101번째 배치 손실: 0.05\n",
      "201번째 배치 손실: 0.06\n",
      "301번째 배치 손실: 0.05\n",
      "401번째 배치 손실: 0.10\n",
      "epoch 13\n",
      "1번째 배치 손실: 0.04\n",
      "101번째 배치 손실: 0.04\n",
      "201번째 배치 손실: 0.04\n",
      "301번째 배치 손실: 0.04\n",
      "401번째 배치 손실: 0.09\n",
      "epoch 14\n",
      "1번째 배치 손실: 0.03\n",
      "101번째 배치 손실: 0.03\n",
      "201번째 배치 손실: 0.03\n",
      "301번째 배치 손실: 0.03\n",
      "401번째 배치 손실: 0.08\n",
      "epoch 15\n",
      "1번째 배치 손실: 0.02\n",
      "101번째 배치 손실: 0.03\n",
      "201번째 배치 손실: 0.02\n",
      "301번째 배치 손실: 0.02\n",
      "401번째 배치 손실: 0.07\n"
     ]
    }
   ],
   "source": [
    "# model fit - mnist\n",
    "\n",
    "fit(model, train_data, train_labels, epochs = 15, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e43b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "predictions = model(test_data)\n",
    "# tensor to numpy\n",
    "predictions = predictions.numpy()\n",
    "pred_labels = np.argmax(predictions, axis = 1)\n",
    "\n",
    "#boolean\n",
    "matches = pred_labels == test_labels\n",
    "# true이면 1 false면 0이 된다\n",
    "print(f'test accuracy: {matches.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b396e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
